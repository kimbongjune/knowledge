"""
RAG íŒŒì´í”„ë¼ì¸
LangChain ê¸°ë°˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±
"""
from typing import List, Optional, Dict, Any, Generator
from pathlib import Path

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from src.vector.vector_manager import VectorManager
from src.llm.model_manager import ModelManager


# í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
DEFAULT_PROMPT_TEMPLATE = """ì•„ë˜ì˜ ë¬¸ë§¥(Context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ê³ , ì—†ìœ¼ë©´ ë‹¹ì‹ ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì¸ì§€ ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ì¸ì§€ êµ¬ë¶„í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.

ë¬¸ë§¥(Context):
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

DOCUMENT_ASSISTANT_PROMPT = """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê¸°ì¡´ ê¸°íšì„œ, ì„¤ê³„ì„œ, ë¶„ì„ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì„œ ì‘ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì œê³µëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
- ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤
- ë¬¸ë§¥ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤
- ê¸°ì¡´ ë¬¸ì„œì˜ ì–‘ì‹ì´ë‚˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ì˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ ì œì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë¬¸ì„œ ì‘ì„±ì— í•„ìš”í•œ í•­ëª©ë“¤ì„ ì•ˆë‚´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ë¬¸ë§¥(Context):
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""


class RAGPipeline:
    """ê²€ìƒ‰ ì¦ê°• ìƒì„± íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self, 
        vector_manager: VectorManager, 
        model_manager: ModelManager,
        prompt_template: Optional[str] = None
    ):
        self.vector_manager = vector_manager
        self.model_manager = model_manager
        self.prompt_template = prompt_template or DOCUMENT_ASSISTANT_PROMPT
        
        self._chain = None
        self._collection_name = None
    
    def setup_chain(
        self, 
        collection_name: str, 
        model_name: Optional[str] = None,
        k: int = 5
    ):
        """
        RAG ì²´ì¸ ì´ˆê¸°í™”
        
        Args:
            collection_name: ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ ì´ë¦„
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ (Noneì´ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸)
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        """
        # LLM ë¡œë“œ
        llm = self.model_manager.get_llm(model_name)
        
        # Retriever ìƒì„±
        retriever = self.vector_manager.get_retriever(
            collection_name, 
            search_kwargs={"k": k}
        )
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["context", "question"]
        )
        
        # LCEL ì²´ì¸ êµ¬ì„±
        def format_docs(docs: List[Document]) -> str:
            return "\n\n---\n\n".join(
                f"[ì¶œì²˜: {doc.metadata.get('filename', 'Unknown')}]\n{doc.page_content}" 
                for doc in docs
            )
        
        self._chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        self._collection_name = collection_name
        print(f"âœ… RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ì»¬ë ‰ì…˜: {collection_name})")
    
    def query(self, question: str) -> str:
        """
        ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
        
        Args:
            question: ì§ˆë¬¸
        
        Returns:
            LLM ì‘ë‹µ
        """
        if not self._chain:
            raise RuntimeError("RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup_chain()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        return self._chain.invoke(question)
    
    def query_with_sources(self, question: str, k: int = 5) -> Dict[str, Any]:
        """
        ì†ŒìŠ¤ ë¬¸ì„œì™€ í•¨ê»˜ ì§ˆì˜ì‘ë‹µ
        
        Args:
            question: ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        
        Returns:
            {"answer": str, "sources": List[Dict]}
        """
        if not self._collection_name:
            raise RuntimeError("RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = self.vector_manager.similarity_search(
            question, 
            self._collection_name, 
            k=k
        )
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n---\n\n".join(
            f"[ì¶œì²˜: {doc.metadata.get('filename', 'Unknown')}]\n{doc.page_content}" 
            for doc in docs
        )
        
        # LLM ì‘ë‹µ ìƒì„±
        prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["context", "question"]
        )
        
        llm = self.model_manager.get_llm()
        formatted_prompt = prompt.format(context=context, question=question)
        
        print(f"ğŸ” ì§ˆë¬¸: {question}")
        print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(docs)}ê°œ")
        
        # OllamaëŠ” max_tokens ëŒ€ì‹  num_predict ì‚¬ìš© (ëª¨ë¸ ì„¤ì •ì—ì„œ ì§€ì •)
        answer = llm.invoke(formatted_prompt)
        
        print(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(answer)}ì)")
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
        sources = []
        seen_sources = set()
        for doc in docs:
            source = doc.metadata.get('source', '')
            if source and source not in seen_sources:
                seen_sources.add(source)
                sources.append({
                    "filename": doc.metadata.get('filename', 'Unknown'),
                    "path": source,
                    "file_type": doc.metadata.get('file_type', 'unknown'),
                    "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                })
        
        return {
            "answer": answer,
            "sources": sources
        }
    
    def stream_query(self, question: str) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        if not self._chain:
            raise RuntimeError("RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        for chunk in self._chain.stream(question):
            yield chunk
    
    def search_similar_documents(
        self, 
        query: str, 
        collection_name: Optional[str] = None,
        k: int = 10
    ) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (RAG ì—†ì´ ê²€ìƒ‰ë§Œ)"""
        coll = collection_name or self._collection_name
        if not coll:
            raise RuntimeError("ì»¬ë ‰ì…˜ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        docs_with_scores = self.vector_manager.similarity_search_with_score(
            query, coll, k=k
        )
        
        results = []
        for doc, score in docs_with_scores:
            results.append({
                "filename": doc.metadata.get('filename', 'Unknown'),
                "path": doc.metadata.get('source', ''),
                "score": float(score),
                "preview": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            })
        
        return results
    
    def set_prompt_template(self, template: str):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ê²½"""
        self.prompt_template = template
        # ì²´ì¸ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ë‹¤ì‹œ ì„¤ì •
        if self._collection_name:
            self.setup_chain(self._collection_name)

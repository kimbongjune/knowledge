"""
LLM ëª¨ë¸ ê´€ë¦¬ì
Ollamaë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ ëª¨ë¸ ê´€ë¦¬ (ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ/ì‚­ì œ)
"""
import json
import requests
from pathlib import Path
from typing import List, Optional, Dict, Any, Generator
from dataclasses import dataclass, asdict

from langchain_community.llms import Ollama


@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    size: str  # ì˜ˆ: "4.7 GB"
    modified: str  # ìˆ˜ì •ì¼
    family: str = ""
    parameter_size: str = ""
    is_vision: bool = False  # ë¹„ì „ ëª¨ë¸ ì—¬ë¶€


class ModelManager:
    """Ollama ê¸°ë°˜ LLM ëª¨ë¸ ê´€ë¦¬"""
    
    OLLAMA_BASE_URL = "http://localhost:11434"
    
    # ë¹„ì „ ëª¨ë¸ ì´ë¦„ íŒ¨í„´
    VISION_MODEL_PATTERNS = ["llava", "bakllava", "moondream", "cogvlm"]
    
    def __init__(self, models_path: Optional[Path] = None):
        self._current_model: Optional[Ollama] = None
        self._current_model_name: Optional[str] = None
    
    def _check_ollama_running(self) -> bool:
        """Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸"""
        try:
            response = requests.get(f"{self.OLLAMA_BASE_URL}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _is_vision_model(self, model_name: str) -> bool:
        """ë¹„ì „ ëª¨ë¸ì¸ì§€ í™•ì¸"""
        name_lower = model_name.lower()
        return any(pattern in name_lower for pattern in self.VISION_MODEL_PATTERNS)
    
    def is_current_model_vision(self) -> bool:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ ë¹„ì „ ëª¨ë¸ì¸ì§€ í™•ì¸"""
        if not self._current_model_name:
            return False
        return self._is_vision_model(self._current_model_name)
    
    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ GBë¡œ í¬ë§·"""
        if size_bytes >= 1024**3:
            return f"{size_bytes / (1024**3):.1f} GB"
        elif size_bytes >= 1024**2:
            return f"{size_bytes / (1024**2):.1f} MB"
        else:
            return f"{size_bytes / 1024:.1f} KB"
    
    def list_installed_models(self) -> List[ModelInfo]:
        """Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ (ì‹¤ì œ ì„¤ì¹˜ëœ ê²ƒë§Œ)"""
        models = []
        try:
            response = requests.get(f"{self.OLLAMA_BASE_URL}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                for m in data.get("models", []):
                    name = m.get("name", "")
                    details = m.get("details", {})
                    models.append(ModelInfo(
                        name=name,
                        size=self._format_size(m.get("size", 0)),
                        modified=m.get("modified_at", "")[:10],  # ë‚ ì§œë§Œ
                        family=details.get("family", ""),
                        parameter_size=details.get("parameter_size", ""),
                        is_vision=self._is_vision_model(name)
                    ))
        except Exception as e:
            print(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return models
    
    def search_models(self, query: str) -> List[Dict]:
        """
        Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë¸ ê²€ìƒ‰
        Note: OllamaëŠ” ê³µì‹ ê²€ìƒ‰ APIê°€ ì—†ì–´ì„œ ì¸ê¸° ëª¨ë¸ ëª©ë¡ ì œê³µ
        """
        # ì¸ê¸°/ì¶”ì²œ ëª¨ë¸ ëª©ë¡ (ìˆ˜ë™ ê´€ë¦¬)
        popular_models = [
            # === 70B+ ëŒ€í˜• ëª¨ë¸ (ë©€í‹° GPU ê¶Œì¥) ===
            {"name": "llama3.3:70b", "description": "Llama 3.3 70B - ìµœì‹  Meta ëª¨ë¸", "size": "43 GB"},
            {"name": "llama3.1:70b", "description": "Llama 3.1 70B - Meta ê³ ì„±ëŠ¥", "size": "40 GB"},
            {"name": "qwen2.5:72b", "description": "Qwen 2.5 72B - ì•Œë¦¬ë°”ë°” ìµœê³ ì„±ëŠ¥, í•œêµ­ì–´ ê°•í•¨", "size": "41 GB"},
            {"name": "qwen2.5:72b-instruct-q4_K_M", "description": "Qwen 2.5 72B Q4 - ìµœì í™” ë²„ì „", "size": "42 GB"},
            {"name": "deepseek-r1:70b", "description": "DeepSeek R1 70B - ì¶”ë¡  ìµœê°•", "size": "40 GB"},
            {"name": "mixtral:8x22b", "description": "Mixtral 8x22B - MoE 141Bê¸‰", "size": "80 GB"},
            {"name": "command-r-plus", "description": "Command-R+ 104B - Cohere ê²€ìƒ‰ íŠ¹í™”", "size": "60 GB"},
            {"name": "wizardlm2:8x22b", "description": "WizardLM 2 8x22B - ì½”ë”©/ìˆ˜í•™ ê°•í™”", "size": "80 GB"},
            {"name": "llama3.2-vision:90b", "description": "Llama 3.2 90B Vision - ê³ ì„±ëŠ¥ ë¹„ì „", "size": "55 GB", "vision": True},
            {"name": "qwen2-vl:72b", "description": "Qwen2 VL 72B - ë¹„ì „ ìµœê³ ì„±ëŠ¥", "size": "41 GB", "vision": True},
            
            # === Qwen ì‹œë¦¬ì¦ˆ ===
            {"name": "qwen2.5:3b", "description": "Qwen 2.5 3B - ê²½ëŸ‰ í•œêµ­ì–´", "size": "1.9 GB"},
            {"name": "qwen2.5:7b", "description": "Qwen 2.5 7B - ë¹ ë¥¸ í•œêµ­ì–´ ì§€ì›", "size": "4.7 GB"},
            {"name": "qwen2.5:14b", "description": "Qwen 2.5 14B - ê· í˜•ì¡íŒ ì„±ëŠ¥", "size": "9.0 GB"},
            {"name": "qwen2.5:32b", "description": "Qwen 2.5 32B - ê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "qwen2.5-coder:7b", "description": "Qwen 2.5 Coder 7B - ì½”ë”© íŠ¹í™”", "size": "4.7 GB"},
            {"name": "qwen2.5-coder:14b", "description": "Qwen 2.5 Coder 14B - ì½”ë”© ê³ ì„±ëŠ¥", "size": "9.0 GB"},
            {"name": "qwen2.5-coder:32b", "description": "Qwen 2.5 Coder 32B - ì½”ë”© ìµœê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "qwen2-vl:7b", "description": "Qwen2 VL 7B - ë¹„ì „ ëª¨ë¸", "size": "4.5 GB", "vision": True},
            
            # === Llama ì‹œë¦¬ì¦ˆ ===
            {"name": "llama3.2:1b", "description": "Llama 3.2 1B - ì´ˆê²½ëŸ‰", "size": "1.3 GB"},
            {"name": "llama3.2:3b", "description": "Llama 3.2 3B - ê²½ëŸ‰ ëª¨ë¸", "size": "2.0 GB"},
            {"name": "llama3.1:8b", "description": "Llama 3.1 8B - ê¸°ë³¸ ì„±ëŠ¥", "size": "4.7 GB"},
            {"name": "llama3.3:70b-instruct-q4_K_M", "description": "Llama 3.3 70B Q4 - ìµœì í™”", "size": "40 GB"},
            {"name": "llama3.2-vision:11b", "description": "Llama 3.2 11B Vision - ì´ë¯¸ì§€ ë¶„ì„", "size": "7.9 GB", "vision": True},
            
            # === DeepSeek ì‹œë¦¬ì¦ˆ ===
            {"name": "deepseek-r1:1.5b", "description": "DeepSeek R1 1.5B - ì´ˆê²½ëŸ‰ ì¶”ë¡ ", "size": "1.1 GB"},
            {"name": "deepseek-r1:7b", "description": "DeepSeek R1 7B - ì¶”ë¡  ëª¨ë¸", "size": "4.7 GB"},
            {"name": "deepseek-r1:8b", "description": "DeepSeek R1 8B - ì¶”ë¡  ëª¨ë¸", "size": "4.9 GB"},
            {"name": "deepseek-r1:14b", "description": "DeepSeek R1 14B - ì¶”ë¡  ê³ ì„±ëŠ¥", "size": "9.0 GB"},
            {"name": "deepseek-r1:32b", "description": "DeepSeek R1 32B - ì¶”ë¡  ìµœê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "deepseek-coder:6.7b", "description": "DeepSeek Coder - ì½”ë”© ì „ë¬¸", "size": "3.8 GB"},
            {"name": "deepseek-coder-v2:16b", "description": "DeepSeek Coder V2 - ì½”ë”© ê³ ì„±ëŠ¥", "size": "9.0 GB"},
            {"name": "deepseek-v2.5:236b", "description": "DeepSeek V2.5 236B - MoE ì´ˆëŒ€í˜•", "size": "131 GB"},
            
            # === Gemma ì‹œë¦¬ì¦ˆ ===
            {"name": "gemma2:2b", "description": "Gemma 2 2B - Google ì´ˆê²½ëŸ‰", "size": "1.6 GB"},
            {"name": "gemma2:9b", "description": "Gemma 2 9B - Google ê²½ëŸ‰ ëª¨ë¸", "size": "5.4 GB"},
            {"name": "gemma2:27b", "description": "Gemma 2 27B - Google ê³ ì„±ëŠ¥", "size": "16 GB"},
            
            # === Mistral ì‹œë¦¬ì¦ˆ ===
            {"name": "mistral:7b", "description": "Mistral 7B - ë¹ ë¥´ê³  íš¨ìœ¨ì ", "size": "4.1 GB"},
            {"name": "mistral-nemo:12b", "description": "Mistral Nemo 12B - ì¤‘ê°„ ì„±ëŠ¥", "size": "7.1 GB"},
            {"name": "mistral-small:22b", "description": "Mistral Small 22B - ì¤‘ìƒê¸‰", "size": "13 GB"},
            {"name": "mistral-large:123b", "description": "Mistral Large 123B - ìµœê³ ì„±ëŠ¥", "size": "69 GB"},
            {"name": "mixtral:8x7b", "description": "Mixtral 8x7B - MoE ê³ ì„±ëŠ¥", "size": "26 GB"},
            
            # === ë¹„ì „ ëª¨ë¸ ===
            {"name": "llava:7b", "description": "LLaVA 7B - ì´ë¯¸ì§€ ë¶„ì„ ê¸°ë³¸", "size": "4.5 GB", "vision": True},
            {"name": "llava:13b", "description": "LLaVA 13B - ì´ë¯¸ì§€ ë¶„ì„ ì„±ëŠ¥", "size": "8.0 GB", "vision": True},
            {"name": "llava:34b", "description": "LLaVA 34B - ê³ ì„±ëŠ¥ ì´ë¯¸ì§€ ë¶„ì„", "size": "20 GB", "vision": True},
            {"name": "bakllava:7b", "description": "BakLLaVA 7B - ê°œì„ ëœ ë¹„ì „", "size": "4.5 GB", "vision": True},
            {"name": "minicpm-v:8b", "description": "MiniCPM-V 8B - ê²½ëŸ‰ ë¹„ì „", "size": "5.5 GB", "vision": True},
            
            # === ì½”ë”© ì „ë¬¸ ===
            {"name": "codellama:7b", "description": "Code Llama 7B - ì½”ë“œ íŠ¹í™”", "size": "3.8 GB"},
            {"name": "codellama:13b", "description": "Code Llama 13B - ì½”ë“œ ê³ ì„±ëŠ¥", "size": "7.4 GB"},
            {"name": "codellama:34b", "description": "Code Llama 34B - ì½”ë“œ ìµœê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "codellama:70b", "description": "Code Llama 70B - ì½”ë“œ ì´ˆê³ ì„±ëŠ¥", "size": "39 GB"},
            {"name": "codegemma:7b", "description": "CodeGemma 7B - Google ì½”ë“œ", "size": "5.0 GB"},
            {"name": "starcoder2:7b", "description": "StarCoder2 7B - ì½”ë“œ ìƒì„±", "size": "4.0 GB"},
            {"name": "starcoder2:15b", "description": "StarCoder2 15B - ì½”ë“œ ê³ ì„±ëŠ¥", "size": "9.0 GB"},
            
            # === ê¸°íƒ€ ì¸ê¸° ëª¨ë¸ ===
            {"name": "phi3:mini", "description": "Phi-3 Mini - MS ì´ˆê²½ëŸ‰", "size": "2.2 GB"},
            {"name": "phi3:medium", "description": "Phi-3 Medium - MS ì¤‘ê°„", "size": "7.9 GB"},
            {"name": "phi3:14b", "description": "Phi-3 14B - MS ê³ ì„±ëŠ¥", "size": "7.9 GB"},
            {"name": "yi:6b", "description": "Yi 6B - ì¤‘êµ­ì–´/ì˜ì–´", "size": "3.5 GB"},
            {"name": "yi:34b", "description": "Yi 34B - ê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "yi-coder:9b", "description": "Yi Coder 9B - ì½”ë”© íŠ¹í™”", "size": "5.0 GB"},
            {"name": "solar:10.7b", "description": "Solar 10.7B - í•œêµ­ì–´ íŠ¹í™”", "size": "6.1 GB"},
            {"name": "solar-pro:22b", "description": "Solar Pro 22B - í•œêµ­ì–´ ê³ ì„±ëŠ¥", "size": "13 GB"},
            {"name": "openchat:7b", "description": "OpenChat 7B - ëŒ€í™” íŠ¹í™”", "size": "4.1 GB"},
            {"name": "neural-chat:7b", "description": "Neural Chat 7B - Intel ìµœì í™”", "size": "4.1 GB"},
            {"name": "dolphin-mixtral:8x7b", "description": "Dolphin Mixtral - ë¬´ê²€ì—´ MoE", "size": "26 GB"},
            {"name": "nous-hermes2:10.7b", "description": "Nous Hermes 2 - ë‹¤ëª©ì ", "size": "6.1 GB"},
            {"name": "orca-mini:7b", "description": "Orca Mini 7B - ê²½ëŸ‰ ì¶”ë¡ ", "size": "4.1 GB"},
            {"name": "vicuna:7b", "description": "Vicuna 7B - ëŒ€í™” íŠ¹í™”", "size": "4.1 GB"},
            {"name": "vicuna:33b", "description": "Vicuna 33B - ëŒ€í™” ê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "wizardlm2:7b", "description": "WizardLM 2 7B - ì§€ì‹œ ë”°ë¥´ê¸°", "size": "4.1 GB"},
            {"name": "falcon:7b", "description": "Falcon 7B - TII ì˜¤í”ˆì†ŒìŠ¤", "size": "4.2 GB"},
            {"name": "falcon:40b", "description": "Falcon 40B - TII ê³ ì„±ëŠ¥", "size": "23 GB"},
            {"name": "falcon:180b", "description": "Falcon 180B - TII ì´ˆëŒ€í˜•", "size": "101 GB"},
        ]
        
        # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
        installed = [m.name for m in self.list_installed_models()]
        
        # ê²€ìƒ‰ì–´ë¡œ í•„í„°ë§
        query_lower = query.lower()
        results = []
        for model in popular_models:
            if query_lower in model["name"].lower() or query_lower in model.get("description", "").lower():
                model["installed"] = model["name"] in installed
                results.append(model)
        
        # ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ ë°˜í™˜
        if not query:
            for model in popular_models:
                model["installed"] = model["name"] in installed
            return popular_models
        
        return results
    
    def pull_model_stream(self, model_name: str) -> Generator[Dict, None, None]:
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ì§„í–‰ë¥ )"""
        try:
            response = requests.post(
                f"{self.OLLAMA_BASE_URL}/api/pull",
                json={"name": model_name, "stream": True},
                stream=True,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        status = data.get("status", "")
                        total = data.get("total", 0)
                        completed = data.get("completed", 0)
                        
                        progress = 0
                        if total > 0:
                            progress = int(completed / total * 100)
                        
                        yield {
                            "status": status,
                            "progress": progress,
                            "completed": completed,
                            "total": total
                        }
                        
                        if status == "success":
                            break
                    except:
                        pass
                        
        except Exception as e:
            yield {"status": "error", "message": str(e)}
    
    def delete_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì‚­ì œ"""
        try:
            response = requests.delete(
                f"{self.OLLAMA_BASE_URL}/api/delete",
                json={"name": model_name},
                timeout=30
            )
            return response.status_code == 200
        except Exception as e:
            print(f"ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, model_name: str, **kwargs) -> Ollama:
        """ëª¨ë¸ ë¡œë“œ"""
        if not self._check_ollama_running():
            raise RuntimeError("Ollamaê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Ollamaë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        # ì´ë¯¸ ê°™ì€ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self._current_model_name == model_name and self._current_model:
            return self._current_model
        
        # LangChain Ollama ì´ˆê¸°í™”
        self._current_model = Ollama(
            base_url=self.OLLAMA_BASE_URL,
            model=model_name,
            temperature=0.7,
            num_ctx=4096,
        )
        self._current_model_name = model_name
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        return self._current_model
    
    def get_llm(self, model_name: Optional[str] = None) -> Ollama:
        """í˜„ì¬ ë¡œë“œëœ LLM ë˜ëŠ” ì§€ì •ëœ ëª¨ë¸ ë°˜í™˜"""
        if model_name:
            return self.load_model(model_name)
        
        if self._current_model:
            return self._current_model
        
        raise RuntimeError("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def unload_model(self):
        """í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self._current_model:
            self._current_model = None
            self._current_model_name = None
            print("ğŸ”„ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def get_current_model_info(self) -> Optional[Dict]:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
        if not self._current_model_name:
            return None
        
        return {
            "name": self._current_model_name,
            "backend": "ollama",
            "is_vision": self._is_vision_model(self._current_model_name)
        }
    
    def is_current_model_vision(self) -> bool:
        """í˜„ì¬ ëª¨ë¸ì´ ë¹„ì „ ëª¨ë¸ì¸ì§€"""
        return self._current_model_name and self._is_vision_model(self._current_model_name)

"""
LLM ëª¨ë¸ ê´€ë¦¬ì
Ollamaë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ ëª¨ë¸ ê´€ë¦¬ (ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ/ì‚­ì œ)
"""
import json
import requests
from pathlib import Path
from typing import List, Optional, Dict, Any, Generator
from dataclasses import dataclass, asdict

from langchain_community.llms import Ollama


@dataclass
class ModelInfo:
    """ëª¨ë¸ ì •ë³´"""
    name: str
    size: str  # ì˜ˆ: "4.7 GB"
    modified: str  # ìˆ˜ì •ì¼
    family: str = ""
    parameter_size: str = ""
    is_vision: bool = False  # ë¹„ì „ ëª¨ë¸ ì—¬ë¶€


class ModelManager:
    """Ollama ê¸°ë°˜ LLM ëª¨ë¸ ê´€ë¦¬"""
    
    OLLAMA_BASE_URL = "http://localhost:11434"
    
    # ë¹„ì „ ëª¨ë¸ ì´ë¦„ íŒ¨í„´
    VISION_MODEL_PATTERNS = ["llava", "bakllava", "moondream", "cogvlm"]
    
    def __init__(self, models_path: Optional[Path] = None):
        self._current_model: Optional[Ollama] = None
        self._current_model_name: Optional[str] = None
    
    def _check_ollama_running(self) -> bool:
        """Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸"""
        try:
            response = requests.get(f"{self.OLLAMA_BASE_URL}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _is_vision_model(self, model_name: str) -> bool:
        """ë¹„ì „ ëª¨ë¸ì¸ì§€ í™•ì¸"""
        name_lower = model_name.lower()
        return any(pattern in name_lower for pattern in self.VISION_MODEL_PATTERNS)
    
    def is_current_model_vision(self) -> bool:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ ë¹„ì „ ëª¨ë¸ì¸ì§€ í™•ì¸"""
        if not self._current_model_name:
            return False
        return self._is_vision_model(self._current_model_name)
    
    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ GBë¡œ í¬ë§·"""
        if size_bytes >= 1024**3:
            return f"{size_bytes / (1024**3):.1f} GB"
        elif size_bytes >= 1024**2:
            return f"{size_bytes / (1024**2):.1f} MB"
        else:
            return f"{size_bytes / 1024:.1f} KB"
    
    def list_installed_models(self) -> List[ModelInfo]:
        """Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ (ì‹¤ì œ ì„¤ì¹˜ëœ ê²ƒë§Œ)"""
        models = []
        try:
            response = requests.get(f"{self.OLLAMA_BASE_URL}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                for m in data.get("models", []):
                    name = m.get("name", "")
                    details = m.get("details", {})
                    models.append(ModelInfo(
                        name=name,
                        size=self._format_size(m.get("size", 0)),
                        modified=m.get("modified_at", "")[:10],  # ë‚ ì§œë§Œ
                        family=details.get("family", ""),
                        parameter_size=details.get("parameter_size", ""),
                        is_vision=self._is_vision_model(name)
                    ))
        except Exception as e:
            print(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return models
    
    def search_models(self, query: str) -> List[Dict]:
        """
        Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë¸ ê²€ìƒ‰
        Note: OllamaëŠ” ê³µì‹ ê²€ìƒ‰ APIê°€ ì—†ì–´ì„œ ì¸ê¸° ëª¨ë¸ ëª©ë¡ ì œê³µ
        """
        # ì¸ê¸°/ì¶”ì²œ ëª¨ë¸ ëª©ë¡ (ìˆ˜ë™ ê´€ë¦¬)
        popular_models = [
            {"name": "qwen2.5:7b", "description": "Qwen 2.5 7B - ë¹ ë¥¸ í•œêµ­ì–´ ì§€ì›", "size": "4.7 GB"},
            {"name": "qwen2.5:14b", "description": "Qwen 2.5 14B - ê· í˜•ì¡íŒ ì„±ëŠ¥", "size": "9.0 GB"},
            {"name": "qwen2.5:32b", "description": "Qwen 2.5 32B - ê³ ì„±ëŠ¥", "size": "19 GB"},
            {"name": "qwen2.5:72b", "description": "Qwen 2.5 72B - ìµœê³  ì„±ëŠ¥", "size": "41 GB"},
            {"name": "llama3.2:3b", "description": "Llama 3.2 3B - ê²½ëŸ‰ ëª¨ë¸", "size": "2.0 GB"},
            {"name": "llama3.2:11b", "description": "Llama 3.2 11B Vision - ì´ë¯¸ì§€ ë¶„ì„", "size": "7.9 GB", "vision": True},
            {"name": "llama3.2:90b", "description": "Llama 3.2 90B Vision - ê³ ì„±ëŠ¥ ë¹„ì „", "size": "55 GB", "vision": True},
            {"name": "llava:7b", "description": "LLaVA 7B - ì´ë¯¸ì§€ ë¶„ì„ ê¸°ë³¸", "size": "4.5 GB", "vision": True},
            {"name": "llava:13b", "description": "LLaVA 13B - ì´ë¯¸ì§€ ë¶„ì„ ì„±ëŠ¥", "size": "8.0 GB", "vision": True},
            {"name": "llava:34b", "description": "LLaVA 34B - ê³ ì„±ëŠ¥ ì´ë¯¸ì§€ ë¶„ì„", "size": "20 GB", "vision": True},
            {"name": "gemma2:9b", "description": "Gemma 2 9B - Google ê²½ëŸ‰ ëª¨ë¸", "size": "5.4 GB"},
            {"name": "gemma2:27b", "description": "Gemma 2 27B - Google ê³ ì„±ëŠ¥", "size": "16 GB"},
            {"name": "mistral:7b", "description": "Mistral 7B - ë¹ ë¥´ê³  íš¨ìœ¨ì ", "size": "4.1 GB"},
            {"name": "codellama:7b", "description": "Code Llama 7B - ì½”ë“œ íŠ¹í™”", "size": "3.8 GB"},
            {"name": "deepseek-coder:6.7b", "description": "DeepSeek Coder - ì½”ë”© ì „ë¬¸", "size": "3.8 GB"},
        ]
        
        # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
        installed = [m.name for m in self.list_installed_models()]
        
        # ê²€ìƒ‰ì–´ë¡œ í•„í„°ë§
        query_lower = query.lower()
        results = []
        for model in popular_models:
            if query_lower in model["name"].lower() or query_lower in model.get("description", "").lower():
                model["installed"] = model["name"] in installed
                results.append(model)
        
        # ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ ë°˜í™˜
        if not query:
            for model in popular_models:
                model["installed"] = model["name"] in installed
            return popular_models
        
        return results
    
    def pull_model_stream(self, model_name: str) -> Generator[Dict, None, None]:
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ì§„í–‰ë¥ )"""
        try:
            response = requests.post(
                f"{self.OLLAMA_BASE_URL}/api/pull",
                json={"name": model_name, "stream": True},
                stream=True,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        status = data.get("status", "")
                        total = data.get("total", 0)
                        completed = data.get("completed", 0)
                        
                        progress = 0
                        if total > 0:
                            progress = int(completed / total * 100)
                        
                        yield {
                            "status": status,
                            "progress": progress,
                            "completed": completed,
                            "total": total
                        }
                        
                        if status == "success":
                            break
                    except:
                        pass
                        
        except Exception as e:
            yield {"status": "error", "message": str(e)}
    
    def delete_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì‚­ì œ"""
        try:
            response = requests.delete(
                f"{self.OLLAMA_BASE_URL}/api/delete",
                json={"name": model_name},
                timeout=30
            )
            return response.status_code == 200
        except Exception as e:
            print(f"ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self, model_name: str, **kwargs) -> Ollama:
        """ëª¨ë¸ ë¡œë“œ"""
        if not self._check_ollama_running():
            raise RuntimeError("Ollamaê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Ollamaë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        # ì´ë¯¸ ê°™ì€ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self._current_model_name == model_name and self._current_model:
            return self._current_model
        
        # LangChain Ollama ì´ˆê¸°í™”
        self._current_model = Ollama(
            base_url=self.OLLAMA_BASE_URL,
            model=model_name,
            temperature=0.7,
            num_ctx=4096,
        )
        self._current_model_name = model_name
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        return self._current_model
    
    def get_llm(self, model_name: Optional[str] = None) -> Ollama:
        """í˜„ì¬ ë¡œë“œëœ LLM ë˜ëŠ” ì§€ì •ëœ ëª¨ë¸ ë°˜í™˜"""
        if model_name:
            return self.load_model(model_name)
        
        if self._current_model:
            return self._current_model
        
        raise RuntimeError("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def unload_model(self):
        """í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self._current_model:
            self._current_model = None
            self._current_model_name = None
            print("ğŸ”„ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def get_current_model_info(self) -> Optional[Dict]:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
        if not self._current_model_name:
            return None
        
        return {
            "name": self._current_model_name,
            "backend": "ollama",
            "is_vision": self._is_vision_model(self._current_model_name)
        }
    
    def is_current_model_vision(self) -> bool:
        """í˜„ì¬ ëª¨ë¸ì´ ë¹„ì „ ëª¨ë¸ì¸ì§€"""
        return self._current_model_name and self._is_vision_model(self._current_model_name)
